# -*- coding: utf-8 -*-
"""Make_Moons_Clasification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uMKOu6eC1yOZgoyL-jXiSGkhSGvXE0SQ
"""

# Ploting
import matplotlib.pyplot as plt
import seaborn as sns
# Pandas
import pandas as pd
# Numpy
import numpy as np
# Tensorflow
import tensorflow as tf
import tensorflow_datasets as tfds
# Sk learn
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

# Make by default all fig sized 10 ,6
plt.rcParams["figure.figsize"] = (10,6)

from sklearn.datasets import make_moons

# Number of examples
n_examples = 3000

# Train and test data sets
X,y = make_moons(n_examples,
                 noise=0.1,
                 random_state=42)
# Check the features
X

# Check the first 10 labels
y[:10]

# Split data into training and testing
X_train , y_train = X[:2500],y[:2500]
X_test, y_test = X[2500:],y[2500:]

# View their shapes
X_train.shape,y_train.shape,X_test.shape,y_test.shape

moons = pd.DataFrame({"X0":X[:,0],
                      "X1":X[:,1],
                      "LABELS":y})
moons.head()

# Time to scatter plot our data
plt.scatter(X[:,0],X[:,1],c=y,cmap=plt.cm.Spectral);

"""# Lets first make some functions to evaluate the model performances"""

def plot_history(history):
  """
  Plots the history curves of the model
  """
  pd.DataFrame(history.history).plot()
  plt.ylabel("Loss")
  plt.xlabel("Epochs")

def plot_decision_boundary(model, X, y):
  """
  Plots the decision boundary created by a model predicting on X.
  """
  # Define the axis boundaries of the plot and create a meshgrid
  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                       np.linspace(y_min, y_max, 100))
  
  # Create X values (we're going to predict on all of these)
  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together
  
  # Make predictions using the trained model
  y_pred = model.predict(x_in)

  # Check for multi-class
  if len(y_pred[0]) > 1:
    print("doing multiclass classification...")
    # We have to reshape our predictions to get them ready for plotting
    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)
  else:
    print("doing binary classifcation...")
    y_pred = np.round(y_pred).reshape(xx.shape)
  
  # Plot decision boundary
  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)
  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())

# Lets make a confusion matrix from scrathc
import itertools

# Create a function for the confugion matrix
def plot_conf_matrix(y_test,y_preds,classes=False,figsize=(15,15),textsize=15):
  """
  Takes as input the y_test and the y_preds
  and then plots a confusion matrix
  """
  # Create the confusion matrix
  cm = confusion_matrix(y_test,tf.round(y_preds))
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:,np.newaxis]
  n_classes = cm.shape[0]

  fig,ax = plt.subplots(figsize=(figsize))
  # Create a matrix plot 
  cax = ax.matshow(cm,cmap=plt.cm.Blues)
  fig.colorbar(cax)

  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])
  
  # Label the axes
  ax.set(title="Confusion matrix",
         xlabel="Predicted Label",
         ylabel="True label",
         xticks=np.arange(n_classes),
         yticks=np.arange(n_classes),
         xticklabels=labels,
         yticklabels=labels)
  
  # Set x-axis labels to bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  # Adjust label size
  ax.yaxis.label.set_size(20)
  ax.xaxis.label.set_size(20)
  ax.title.set_size(20)

  # Set threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i , j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
    plt.text(j,i,f"{cm[i,j]} ({cm_norm[i,j]*100:.1f}%)",horizontalalignment="center",
             color="white" if cm[i,j] > threshold else "black",
             size=textsize)
  plt.show()

"""# Time to build our models"""

# Build first model with 5 hidden layers and 1 output layer

# Set up random seed
tf.random.set_seed(42)

# Build model
model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

# Compile the model
model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(),
                metrics=["accuracy"])


# Fit the model
history_1 = model_1.fit(X_train,
                        y_train,
                        epochs=100,
                        verbose = 0)

# Evaluate the model
model_1.evaluate(X_test,y_test)

plot_history(history_1)

plot_decision_boundary(model_1,X_test,y_test)

plot_decision_boundary(model_1,X_train,y_train)

y_preds = model_1.predict(X_test)
plot_conf_matrix(y_test,y_preds,classes = [0,1])

"""## Lets see if we can improve the model any further"""

# Build a moel to find the best learning rate
# Set up random seed
tf.random.set_seed(42)

# BUild the model
model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(1,activation="sigmoid"),
])


# Compile the model
model_2.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(),
                metrics=["accuracy"])


# Setup lr scheduler 
lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 **(epoch/20))

# Fit the model
history_2 = model_2.fit(X_train,
                        y_train,
                        epochs = 100,
                        callbacks = [lr_schedule],
                        verbose = 0)

plot_history(history_2)

lrs = 1e-4 * (10 ** (tf.range(100)/20))
plt.semilogx(lrs,history_2.history["loss"])

"""As we can see the perfect learning rate is :10 ** -3 (0.001) (default adam lr = 0.001) so lets build a model with the perfect lr and a callback function"""

# Build best model
# Set up random seed
tf.random.set_seed(42)

# Build model
model_3 = tf.keras.Sequential([
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Dense(6,activation="relu"),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1,activation="sigmoid"),
])

# Compile the model
model_3.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
                metrics=["accuracy"])

# Set up callback
callback = tf.keras.callbacks.EarlyStopping(monitor="accuracy",patience=5)

# Fit the model
history_3 = model_3.fit(X_train,
                        y_train,
                        epochs = 400,
                        callbacks=[callback],
                        verbose=0)

# Evaluate the model 
model_3.evaluate(X_test,y_test)

plot_history(history_3)

plot_decision_boundary(model_3,X,y)

y_preds = model_3.predict(X_test)
plot_conf_matrix(y_test,y_preds)

loss_1 , accuracy_1 = model_1.evaluate(X_test,y_test)
loss_2 , accuracy_2 = model_2.evaluate(X_test,y_test)
loss_3 , accuracy_3 = model_3.evaluate(X_test,y_test)

model_results = [["model 1",loss_1,accuracy_1],
                 ["model 2",loss_2,accuracy_2],
                 ["model 3",loss_3,accuracy_3]]
model_results

# Turn results into a dataframe
model_results_df = pd.DataFrame(model_results,columns=["model","loss","accuracy"])
model_results_df

"""## Time to plot the model results"""

model_results_df.plot(x="model",y="loss",kind="bar")
plt.xticks(rotation= "horizontal")
plt.title("Lower is better");

model_results_df.plot(x="model",y="accuracy",kind="bar")
plt.xticks(rotation="horizontal")
plt.title("Higher is better");

