# -*- coding: utf-8 -*-
"""Transfer_Learning_Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16I-3DmFvi9thenUBi-0UA4gebmFDaJdm

## Imports
"""

import numpy as np
import pandas as pd

import matplotlib.image as mpimg
import matplotlib.pyplot as plt


import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator


import pathlib
import zipfile
import os
import random
import datetime

from google.colab import files
files.upload()

# Check if you upload successful
!ls -lha kaggle.json
# Output: -rw-r--r-- 1 root root 63 Aug  8 11:17 kaggle.json

# Install kaggle package
!pip install -q kaggle

"""## Make kaggle directory and download dataset"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Start to download dataset
!kaggle datasets download -d prasunroy/natural-images

"""## Unzip and view dataset"""

zip_ref = zipfile.ZipFile("natural-images.zip")
zip_ref.extractall()
zip_ref.close()

# How many images in each folder
for dirpath,dirnames,filenames in os.walk('/content/natural_images'):
  if len(dirnames) == 0:
    print(f"There are {len(filenames)} images in {dirpath} ")
  else:
    print(f"There are {len(dirnames)} directories in {dirpath}")

# Get the subdirectories (class_names)
data_dir = pathlib.Path('/content/natural_images')
class_names = np.array(sorted(item.name for item in data_dir.glob("*")))
print(class_names)

"""## Functions"""

# Make a function to view random images for each class
def view_images(target_dir):
  """
  Plots out a random image for each class
  """
  plt.figure(figsize=(15,15))
  for i in range(len(class_names)):
    plt.subplot(4,4,i+1)
    target_folder = target_dir + "/" + class_names[i]
    random_image = random.sample(os.listdir(target_folder),1)
    img = mpimg.imread(target_folder + "/" + random_image[0])
    plt.imshow(img)
    plt.axis("off")
    img_info = class_names[i] + "\n" + str(img.shape)
    plt.title(img_info)

def create_tensorboard_callback(dir_name,experiment_name):
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%d-%m-%Y|%H:%M")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir = log_dir
  )
  print(f"Saving Tensorboard experiment log files to : {log_dir}")
  return tensorboard_callback

# Lets make a function to create a model from url
def create_model(model_url,num_classes = len(class_names)):
  """
  Creates a keras model from a tf hub url

  Returns:
    An uncompiled Keras sequential model
  """
  # Download the pretained model and save it as a keras layer
  feature_extractor_layer = hub.KerasLayer(model_url,
                                           trainable = False,
                                           name = "feature_extractor_layer",
                                           input_shape = IMAGE_SHAPE + (3,))
  
  # Create our own model
  model = tf.keras.Sequential([
    feature_extractor_layer,
    layers.Dense(num_classes,activation="softmax",name="Output_layer")
  ])
  return model

# Function to view loss and accuracy of a model
def plot_loss_curves(history):
  """
  Plots out separate loss curves for training and validation data
  """
  loss = history.history["loss"]
  accuracy = history.history["accuracy"]
  epochs = range(len(history.history["loss"]))

  # Plot the loss
  plt.plot(epochs,loss,label="training loss")
  plt.title("Loss")
  plt.xlabel("Epochs")
  plt.legend()

  # Plot out the accuracy
  plt.figure()
  plt.plot(epochs,accuracy,label="Training accuracy")
  plt.title("Accuracy")
  plt.xlabel("Epochs")
  plt.legend();

view_images("/content/natural_images")

"""## Setting up data for the model training"""

# Setup global variables
IMAGE_SHAPE = (512,512)
BATCH_SIZE = 32
EPOCHS = 2

train_dir = "/content/natural_images"

train_datagen = ImageDataGenerator(rescale = 1/255.)

train_data = train_datagen.flow_from_directory(train_dir,
                                               target_size = IMAGE_SHAPE,
                                               batch_size = BATCH_SIZE,
                                               class_mode = "categorical")

efficientnet_url = "https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1"

# Create model
efficientnet_model = create_model(model_url = efficientnet_url)

# Compile the model
efficientnet_model.compile(loss = "categorical_crossentropy",
                           optimizer = tf.keras.optimizers.Adam(),
                           metrics = ["accuracy"])

efficient_history = efficientnet_model.fit(train_data,
                                           epochs = EPOCHS,
                                           steps_per_epoch = len(train_data),
                                           callbacks = [create_tensorboard_callback(dir_name= "Tf_hub_models",
                                                                                    experiment_name = "efficientnetB7V1")])

plot_loss_curves(efficient_history)

efficientnet_model.summary()